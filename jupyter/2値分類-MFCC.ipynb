{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2faad08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 17:30:59.245540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-24 17:30:59.245593: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f343d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.21568628, 0.03137255, 0.        ],\n",
       "         [0.21568628, 0.03137255, 0.        ],\n",
       "         [0.21176471, 0.03529412, 0.        ],\n",
       "         ...,\n",
       "         [0.21960784, 0.04705882, 0.        ],\n",
       "         [0.21176471, 0.03921569, 0.        ],\n",
       "         [0.1882353 , 0.01960784, 0.        ]],\n",
       "\n",
       "        [[0.3137255 , 0.15294118, 0.04313726],\n",
       "         [0.3137255 , 0.15294118, 0.04313726],\n",
       "         [0.30980393, 0.15686275, 0.04313726],\n",
       "         ...,\n",
       "         [0.31764707, 0.16862746, 0.03921569],\n",
       "         [0.30980393, 0.16078432, 0.03921569],\n",
       "         [0.2901961 , 0.14117648, 0.03921569]],\n",
       "\n",
       "        [[0.5921569 , 0.4745098 , 0.3254902 ],\n",
       "         [0.5921569 , 0.4745098 , 0.3254902 ],\n",
       "         [0.5882353 , 0.47843137, 0.3254902 ],\n",
       "         ...,\n",
       "         [0.59607846, 0.4862745 , 0.31764707],\n",
       "         [0.5882353 , 0.48235294, 0.31764707],\n",
       "         [0.5803922 , 0.47058824, 0.31764707]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.20784314, 0.7176471 , 0.4745098 ],\n",
       "         [0.20784314, 0.7176471 , 0.4745098 ],\n",
       "         [0.20784314, 0.7176471 , 0.4745098 ],\n",
       "         ...,\n",
       "         [0.20784314, 0.7176471 , 0.46666667],\n",
       "         [0.21176471, 0.72156864, 0.47058824],\n",
       "         [0.20784314, 0.7176471 , 0.46666667]],\n",
       "\n",
       "        [[0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         [0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         [0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         ...,\n",
       "         [0.22352941, 0.73333335, 0.48235294],\n",
       "         [0.22352941, 0.73333335, 0.48235294],\n",
       "         [0.22352941, 0.73333335, 0.48235294]],\n",
       "\n",
       "        [[0.22352941, 0.73333335, 0.49019608],\n",
       "         [0.22352941, 0.73333335, 0.49019608],\n",
       "         [0.22352941, 0.73333335, 0.49019608],\n",
       "         ...,\n",
       "         [0.22745098, 0.7372549 , 0.4862745 ],\n",
       "         [0.23137255, 0.7411765 , 0.49019608],\n",
       "         [0.22745098, 0.7372549 , 0.4862745 ]]],\n",
       "\n",
       "\n",
       "       [[[0.01176471, 0.12156863, 0.        ],\n",
       "         [0.10588235, 0.21568628, 0.07058824],\n",
       "         [0.23137255, 0.33333334, 0.18431373],\n",
       "         ...,\n",
       "         [0.2       , 0.02352941, 0.        ],\n",
       "         [0.2       , 0.02352941, 0.        ],\n",
       "         [0.19607843, 0.02352941, 0.        ]],\n",
       "\n",
       "        [[0.11372549, 0.23529412, 0.08627451],\n",
       "         [0.18431373, 0.30588236, 0.15686275],\n",
       "         [0.28235295, 0.39607844, 0.24313726],\n",
       "         ...,\n",
       "         [0.29803923, 0.14509805, 0.03921569],\n",
       "         [0.29803923, 0.14509805, 0.03921569],\n",
       "         [0.29803923, 0.14509805, 0.03921569]],\n",
       "\n",
       "        [[0.39215687, 0.5411765 , 0.38039216],\n",
       "         [0.40392157, 0.5529412 , 0.39215687],\n",
       "         [0.42745098, 0.5686275 , 0.40784314],\n",
       "         ...,\n",
       "         [0.5764706 , 0.46666667, 0.31764707],\n",
       "         [0.5764706 , 0.46666667, 0.31764707],\n",
       "         [0.58431375, 0.4745098 , 0.3254902 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.19607843, 0.7058824 , 0.47058824],\n",
       "         [0.2       , 0.70980394, 0.4745098 ],\n",
       "         [0.20392157, 0.7137255 , 0.47843137],\n",
       "         ...,\n",
       "         [0.19215687, 0.70980394, 0.48235294],\n",
       "         [0.19215687, 0.70980394, 0.48235294],\n",
       "         [0.1882353 , 0.7058824 , 0.48235294]],\n",
       "\n",
       "        [[0.21176471, 0.72156864, 0.4862745 ],\n",
       "         [0.21568628, 0.7254902 , 0.49019608],\n",
       "         [0.21960784, 0.7294118 , 0.49411765],\n",
       "         ...,\n",
       "         [0.20392157, 0.72156864, 0.49411765],\n",
       "         [0.20392157, 0.72156864, 0.49411765],\n",
       "         [0.2       , 0.7176471 , 0.49411765]],\n",
       "\n",
       "        [[0.21568628, 0.7254902 , 0.49019608],\n",
       "         [0.21960784, 0.7294118 , 0.49411765],\n",
       "         [0.22352941, 0.73333335, 0.49803922],\n",
       "         ...,\n",
       "         [0.20784314, 0.7254902 , 0.49803922],\n",
       "         [0.20784314, 0.7254902 , 0.49803922],\n",
       "         [0.20392157, 0.72156864, 0.49803922]]],\n",
       "\n",
       "\n",
       "       [[[0.05098039, 0.05882353, 0.        ],\n",
       "         [0.08627451, 0.10980392, 0.        ],\n",
       "         [0.13333334, 0.21960784, 0.        ],\n",
       "         ...,\n",
       "         [0.21960784, 0.04705882, 0.        ],\n",
       "         [0.21568628, 0.04313726, 0.        ],\n",
       "         [0.20784314, 0.03529412, 0.        ]],\n",
       "\n",
       "        [[0.16078432, 0.18039216, 0.04313726],\n",
       "         [0.18431373, 0.22352941, 0.03921569],\n",
       "         [0.21176471, 0.30980393, 0.07450981],\n",
       "         ...,\n",
       "         [0.31764707, 0.16862746, 0.03921569],\n",
       "         [0.3137255 , 0.16470589, 0.03921569],\n",
       "         [0.30588236, 0.15686275, 0.03921569]],\n",
       "\n",
       "        [[0.46666667, 0.5137255 , 0.34117648],\n",
       "         [0.45882353, 0.5254902 , 0.32156864],\n",
       "         [0.42352942, 0.5529412 , 0.30980393],\n",
       "         ...,\n",
       "         [0.59607846, 0.4862745 , 0.3137255 ],\n",
       "         [0.5921569 , 0.4862745 , 0.3137255 ],\n",
       "         [0.5882353 , 0.48235294, 0.3137255 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.22352941, 0.72156864, 0.45882353],\n",
       "         [0.22352941, 0.72156864, 0.45882353],\n",
       "         [0.21960784, 0.7176471 , 0.45490196],\n",
       "         ...,\n",
       "         [0.22352941, 0.72156864, 0.45882353],\n",
       "         [0.22352941, 0.72156864, 0.45882353],\n",
       "         [0.22352941, 0.72156864, 0.45882353]],\n",
       "\n",
       "        [[0.23921569, 0.7372549 , 0.4745098 ],\n",
       "         [0.23529412, 0.73333335, 0.47058824],\n",
       "         [0.23137255, 0.7294118 , 0.46666667],\n",
       "         ...,\n",
       "         [0.23921569, 0.7372549 , 0.4745098 ],\n",
       "         [0.23921569, 0.7372549 , 0.4745098 ],\n",
       "         [0.23921569, 0.7372549 , 0.4745098 ]],\n",
       "\n",
       "        [[0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.23921569, 0.7372549 , 0.4745098 ],\n",
       "         [0.23529412, 0.73333335, 0.47058824],\n",
       "         ...,\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.24313726, 0.7411765 , 0.47843137]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.21568628, 0.0627451 , 0.        ],\n",
       "         [0.21568628, 0.0627451 , 0.        ],\n",
       "         [0.21568628, 0.0627451 , 0.        ],\n",
       "         ...,\n",
       "         [0.2       , 0.07058824, 0.        ],\n",
       "         [0.19607843, 0.05882353, 0.        ],\n",
       "         [0.19607843, 0.05490196, 0.        ]],\n",
       "\n",
       "        [[0.3137255 , 0.18039216, 0.03921569],\n",
       "         [0.3137255 , 0.18039216, 0.03921569],\n",
       "         [0.3137255 , 0.18039216, 0.03921569],\n",
       "         ...,\n",
       "         [0.29803923, 0.19215687, 0.04313726],\n",
       "         [0.29803923, 0.18039216, 0.04313726],\n",
       "         [0.29411766, 0.1764706 , 0.04313726]],\n",
       "\n",
       "        [[0.5882353 , 0.49411765, 0.32156864],\n",
       "         [0.5882353 , 0.49411765, 0.32156864],\n",
       "         [0.5882353 , 0.49411765, 0.32156864],\n",
       "         ...,\n",
       "         [0.5803922 , 0.5176471 , 0.34117648],\n",
       "         [0.58431375, 0.5058824 , 0.3372549 ],\n",
       "         [0.5764706 , 0.49803922, 0.3254902 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.22352941, 0.72156864, 0.4509804 ],\n",
       "         [0.22352941, 0.72156864, 0.4509804 ],\n",
       "         [0.22352941, 0.72156864, 0.4509804 ],\n",
       "         ...,\n",
       "         [0.22745098, 0.7254902 , 0.4627451 ],\n",
       "         [0.22745098, 0.7254902 , 0.4627451 ],\n",
       "         [0.22352941, 0.72156864, 0.45882353]],\n",
       "\n",
       "        [[0.23921569, 0.7372549 , 0.46666667],\n",
       "         [0.23921569, 0.7372549 , 0.46666667],\n",
       "         [0.23921569, 0.7372549 , 0.46666667],\n",
       "         ...,\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.23921569, 0.7372549 , 0.4745098 ]],\n",
       "\n",
       "        [[0.24313726, 0.7411765 , 0.47058824],\n",
       "         [0.24313726, 0.7411765 , 0.47058824],\n",
       "         [0.24313726, 0.7411765 , 0.47058824],\n",
       "         ...,\n",
       "         [0.24705882, 0.74509805, 0.48235294],\n",
       "         [0.24705882, 0.74509805, 0.48235294],\n",
       "         [0.24313726, 0.7411765 , 0.47843137]]],\n",
       "\n",
       "\n",
       "       [[[0.24313726, 0.02745098, 0.        ],\n",
       "         [0.24705882, 0.03137255, 0.        ],\n",
       "         [0.2509804 , 0.03529412, 0.        ],\n",
       "         ...,\n",
       "         [0.20392157, 0.03921569, 0.        ],\n",
       "         [0.20392157, 0.03921569, 0.        ],\n",
       "         [0.20392157, 0.03921569, 0.        ]],\n",
       "\n",
       "        [[0.34117648, 0.14901961, 0.03921569],\n",
       "         [0.34509805, 0.15294118, 0.03921569],\n",
       "         [0.34901962, 0.15686275, 0.03921569],\n",
       "         ...,\n",
       "         [0.3019608 , 0.16078432, 0.03921569],\n",
       "         [0.3019608 , 0.16078432, 0.03921569],\n",
       "         [0.3019608 , 0.16078432, 0.03921569]],\n",
       "\n",
       "        [[0.6117647 , 0.47058824, 0.30588236],\n",
       "         [0.6156863 , 0.4745098 , 0.30588236],\n",
       "         [0.61960787, 0.4745098 , 0.30980393],\n",
       "         ...,\n",
       "         [0.58431375, 0.48235294, 0.32156864],\n",
       "         [0.58431375, 0.48235294, 0.32156864],\n",
       "         [0.5803922 , 0.47843137, 0.31764707]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.22745098, 0.7254902 , 0.4627451 ],\n",
       "         [0.22745098, 0.7254902 , 0.4627451 ],\n",
       "         [0.22745098, 0.7254902 , 0.4627451 ],\n",
       "         ...,\n",
       "         [0.23137255, 0.72156864, 0.45490196],\n",
       "         [0.23137255, 0.72156864, 0.45490196],\n",
       "         [0.23529412, 0.7294118 , 0.4509804 ]],\n",
       "\n",
       "        [[0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         [0.24313726, 0.7411765 , 0.47843137],\n",
       "         ...,\n",
       "         [0.24705882, 0.7372549 , 0.47058824],\n",
       "         [0.24705882, 0.7372549 , 0.47058824],\n",
       "         [0.2509804 , 0.74509805, 0.46666667]],\n",
       "\n",
       "        [[0.24705882, 0.74509805, 0.48235294],\n",
       "         [0.24705882, 0.74509805, 0.48235294],\n",
       "         [0.24705882, 0.74509805, 0.48235294],\n",
       "         ...,\n",
       "         [0.2509804 , 0.7411765 , 0.4745098 ],\n",
       "         [0.2509804 , 0.7411765 , 0.4745098 ],\n",
       "         [0.25490198, 0.7490196 , 0.47058824]]],\n",
       "\n",
       "\n",
       "       [[[0.23137255, 0.15294118, 0.        ],\n",
       "         [0.23137255, 0.15294118, 0.        ],\n",
       "         [0.23137255, 0.15294118, 0.        ],\n",
       "         ...,\n",
       "         [0.2       , 0.30980393, 0.08235294],\n",
       "         [0.18039216, 0.2901961 , 0.05490196],\n",
       "         [0.22352941, 0.34509805, 0.09411765]],\n",
       "\n",
       "        [[0.31764707, 0.25490198, 0.05098039],\n",
       "         [0.31764707, 0.25490198, 0.05098039],\n",
       "         [0.31764707, 0.25490198, 0.05098039],\n",
       "         ...,\n",
       "         [0.27058825, 0.39215687, 0.16078432],\n",
       "         [0.25490198, 0.38039216, 0.14117648],\n",
       "         [0.2901961 , 0.42352942, 0.17254902]],\n",
       "\n",
       "        [[0.5529412 , 0.53333336, 0.31764707],\n",
       "         [0.5529412 , 0.53333336, 0.31764707],\n",
       "         [0.5529412 , 0.53333336, 0.31764707],\n",
       "         ...,\n",
       "         [0.46666667, 0.6156863 , 0.3764706 ],\n",
       "         [0.4627451 , 0.61960787, 0.37254903],\n",
       "         [0.47058824, 0.627451  , 0.37254903]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.20392157, 0.7137255 , 0.47058824],\n",
       "         [0.20392157, 0.7137255 , 0.47058824],\n",
       "         [0.20392157, 0.7137255 , 0.47058824],\n",
       "         ...,\n",
       "         [0.21176471, 0.72156864, 0.47058824],\n",
       "         [0.21176471, 0.72156864, 0.47058824],\n",
       "         [0.20392157, 0.7137255 , 0.4627451 ]],\n",
       "\n",
       "        [[0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         [0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         [0.21960784, 0.7294118 , 0.4862745 ],\n",
       "         ...,\n",
       "         [0.22352941, 0.73333335, 0.48235294],\n",
       "         [0.22352941, 0.73333335, 0.48235294],\n",
       "         [0.21568628, 0.7254902 , 0.4745098 ]],\n",
       "\n",
       "        [[0.22352941, 0.73333335, 0.49019608],\n",
       "         [0.22352941, 0.73333335, 0.49019608],\n",
       "         [0.22352941, 0.73333335, 0.49019608],\n",
       "         ...,\n",
       "         [0.22745098, 0.7372549 , 0.4862745 ],\n",
       "         [0.22745098, 0.7372549 , 0.4862745 ],\n",
       "         [0.21960784, 0.7294118 , 0.47843137]]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "f = pd.read_csv('./train_images.csv',header=1)\n",
    "f = f.sample(frac=1).reset_index(drop=True)\n",
    "x_lavel  = f.iloc[:,1:]\n",
    "x_test = f.iloc[:,0]\n",
    "_x_test = np.array(x_test)\n",
    "_x_lavel = np.array(x_lavel)\n",
    "\n",
    "train_image = []\n",
    "\n",
    "for line in _x_test:\n",
    "    line = line.rstrip()\n",
    "    l = line.split()\n",
    "    # データを読み込んで50x50に拡大\n",
    "    img = Image.open(l[0])\n",
    "    img = img.resize((50, 50))\n",
    "    # 一列にした後、0-1のfloat値にする\n",
    "    _img = np.asarray(img)\n",
    "    \n",
    "    train_image.append(_img.astype(np.float32)/255.0)\n",
    "    #check_image.append(_img)\n",
    "    # ラベルを1-of-k方式で用意する\n",
    "    #train_label.append(tmp)\n",
    "# numpy形式に変換\n",
    "train_image = np.asarray(train_image)\n",
    "\n",
    "\n",
    "train_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a99e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "x_one_hot = np_utils.to_categorical(_x_lavel)\n",
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 教師データとテストデータに分割\n",
    "x_train, x_test, y_train, y_test= train_test_split(train_image, x_one_hot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5a730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (1, 1), activation='relu', input_shape=(50, 50, 3)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (1, 1), activation='relu'))\n",
    "model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
    "model.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(7744, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b349a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fea8d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 50, 50, 32)        128       \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 24, 24, 32)        1056      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 24, 24, 64)        2112      \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 23, 23, 64)        16448     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 11, 11, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 11, 11, 64)        4160      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 7744)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 7744)              59977280  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               3965440   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,976,898\n",
      "Trainable params: 63,976,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b25e682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 8s 400ms/step - loss: 0.6719 - accuracy: 0.6996 - val_loss: 0.6190 - val_accuracy: 0.7258\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 6s 361ms/step - loss: 0.6228 - accuracy: 0.7158 - val_loss: 0.6133 - val_accuracy: 0.7258\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 7s 402ms/step - loss: 0.6083 - accuracy: 0.7158 - val_loss: 0.5935 - val_accuracy: 0.7258\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 8s 428ms/step - loss: 0.6046 - accuracy: 0.7158 - val_loss: 0.5901 - val_accuracy: 0.7258\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 7s 389ms/step - loss: 0.5985 - accuracy: 0.7158 - val_loss: 0.5867 - val_accuracy: 0.7258\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 7s 365ms/step - loss: 0.6044 - accuracy: 0.7158 - val_loss: 0.6107 - val_accuracy: 0.7258\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 7s 361ms/step - loss: 0.6012 - accuracy: 0.7158 - val_loss: 0.5868 - val_accuracy: 0.7258\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 7s 363ms/step - loss: 0.5966 - accuracy: 0.7158 - val_loss: 0.5882 - val_accuracy: 0.7258\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 7s 398ms/step - loss: 0.5995 - accuracy: 0.7158 - val_loss: 0.5816 - val_accuracy: 0.7258\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 7s 398ms/step - loss: 0.6045 - accuracy: 0.7158 - val_loss: 0.5935 - val_accuracy: 0.7258\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 8s 435ms/step - loss: 0.5995 - accuracy: 0.7158 - val_loss: 0.5854 - val_accuracy: 0.7258\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 7s 398ms/step - loss: 0.6034 - accuracy: 0.7158 - val_loss: 0.5826 - val_accuracy: 0.7258\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.5957 - accuracy: 0.7158 - val_loss: 0.5819 - val_accuracy: 0.7258\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 7s 391ms/step - loss: 0.5887 - accuracy: 0.7158 - val_loss: 0.5816 - val_accuracy: 0.7258\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 6s 364ms/step - loss: 0.5880 - accuracy: 0.7158 - val_loss: 0.5829 - val_accuracy: 0.7258\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.5847 - accuracy: 0.7158 - val_loss: 0.5843 - val_accuracy: 0.7258\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 8s 423ms/step - loss: 0.5907 - accuracy: 0.7158 - val_loss: 0.5857 - val_accuracy: 0.7258\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 7s 385ms/step - loss: 0.5769 - accuracy: 0.7158 - val_loss: 0.5845 - val_accuracy: 0.7258\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 7s 413ms/step - loss: 0.5848 - accuracy: 0.7158 - val_loss: 0.5842 - val_accuracy: 0.7258\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 7s 384ms/step - loss: 0.5736 - accuracy: 0.7158 - val_loss: 0.6032 - val_accuracy: 0.7258\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 8s 421ms/step - loss: 0.5830 - accuracy: 0.7158 - val_loss: 0.5908 - val_accuracy: 0.7258\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 7s 401ms/step - loss: 0.5731 - accuracy: 0.7158 - val_loss: 0.5824 - val_accuracy: 0.7258\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 7s 402ms/step - loss: 0.5738 - accuracy: 0.7158 - val_loss: 0.5769 - val_accuracy: 0.7258\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 7s 384ms/step - loss: 0.5713 - accuracy: 0.7158 - val_loss: 0.5717 - val_accuracy: 0.7258\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 7s 385ms/step - loss: 0.5755 - accuracy: 0.7158 - val_loss: 0.5700 - val_accuracy: 0.7258\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 7s 418ms/step - loss: 0.5719 - accuracy: 0.7158 - val_loss: 0.5928 - val_accuracy: 0.7258\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 8s 434ms/step - loss: 0.6052 - accuracy: 0.7158 - val_loss: 0.5721 - val_accuracy: 0.7258\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 7s 381ms/step - loss: 0.5765 - accuracy: 0.7158 - val_loss: 0.5756 - val_accuracy: 0.7258\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 7s 391ms/step - loss: 0.5843 - accuracy: 0.7158 - val_loss: 0.5882 - val_accuracy: 0.7258\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 7s 403ms/step - loss: 0.5975 - accuracy: 0.7158 - val_loss: 0.5743 - val_accuracy: 0.7258\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 8s 425ms/step - loss: 0.5706 - accuracy: 0.7158 - val_loss: 0.5693 - val_accuracy: 0.7258\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 7s 396ms/step - loss: 0.5682 - accuracy: 0.7158 - val_loss: 0.5733 - val_accuracy: 0.7258\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 8s 429ms/step - loss: 0.5679 - accuracy: 0.7158 - val_loss: 0.5704 - val_accuracy: 0.7258\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.5669 - accuracy: 0.7158 - val_loss: 0.5685 - val_accuracy: 0.7258\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 8s 427ms/step - loss: 0.5621 - accuracy: 0.7158 - val_loss: 0.6089 - val_accuracy: 0.7258\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 9s 472ms/step - loss: 0.5675 - accuracy: 0.7176 - val_loss: 0.5684 - val_accuracy: 0.7258\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 7s 389ms/step - loss: 0.5614 - accuracy: 0.7158 - val_loss: 0.5643 - val_accuracy: 0.7258\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 7s 396ms/step - loss: 0.5660 - accuracy: 0.7194 - val_loss: 0.5681 - val_accuracy: 0.7419\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 7s 399ms/step - loss: 0.5661 - accuracy: 0.7194 - val_loss: 0.5646 - val_accuracy: 0.7258\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 7s 418ms/step - loss: 0.5548 - accuracy: 0.7212 - val_loss: 0.5747 - val_accuracy: 0.7419\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 7s 370ms/step - loss: 0.5517 - accuracy: 0.7320 - val_loss: 0.5495 - val_accuracy: 0.7581\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 7s 387ms/step - loss: 0.5554 - accuracy: 0.7248 - val_loss: 0.5655 - val_accuracy: 0.7258\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 7s 387ms/step - loss: 0.5494 - accuracy: 0.7248 - val_loss: 0.5676 - val_accuracy: 0.7258\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 7s 409ms/step - loss: 0.5463 - accuracy: 0.7320 - val_loss: 0.5631 - val_accuracy: 0.7258\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 8s 420ms/step - loss: 0.5422 - accuracy: 0.7266 - val_loss: 0.5713 - val_accuracy: 0.7258\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 7s 396ms/step - loss: 0.5365 - accuracy: 0.7284 - val_loss: 0.5637 - val_accuracy: 0.7258\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 7s 402ms/step - loss: 0.5418 - accuracy: 0.7158 - val_loss: 0.5502 - val_accuracy: 0.7258\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 7s 407ms/step - loss: 0.5405 - accuracy: 0.7428 - val_loss: 0.5463 - val_accuracy: 0.7258\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.5386 - accuracy: 0.7428 - val_loss: 0.5935 - val_accuracy: 0.6613\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 7s 400ms/step - loss: 0.6011 - accuracy: 0.7068 - val_loss: 0.5974 - val_accuracy: 0.7258\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e09dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"accuracy\"], label=\"acc\")\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test score\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('enter_model_mfcc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7468ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.1202 - accuracy: 0.9548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12020426988601685, 0.9548386931419373]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "load_model = keras.models.load_model('enter_model_mfcc.h5')\n",
    "\n",
    "load_model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge = load_model.predict(x_test, batch_size=32, verbose=0, steps=None)\n",
    "\n",
    "for i in range(160):\n",
    "    if hoge[i][0] >= 0.6:\n",
    "        print(hoge[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e485f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 3)\n",
      "(1, 50, 50, 3)\n",
      "[[0.02381883 0.9761812 ]]\n"
     ]
    }
   ],
   "source": [
    "# ※※※　　　　　　　　　　　　　　　　　　　　　　　　　　※※※\n",
    "# 推論試験用、学習データ作成、試験時はこのブロックは実行しないこと \n",
    "# ※※※　　　　　　　　　　　　　　　　　　　　　　　　　　※※※\n",
    "\n",
    "img = Image.open('./capcha.jpg')\n",
    "img_ = np.asarray(img.resize((50,50)))\n",
    "print(img_.shape)\n",
    "mfcc_ = np.asarray([img_.astype(np.float32)/255.0])\n",
    "print(mfcc_.shape)\n",
    "hoge2 = load_model.predict(mfcc_, batch_size=32, verbose=0, steps=None)\n",
    "\n",
    "print(hoge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7c235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b00fd44d87c528ae857fa86a0704c55dde2665c80c2cbc47954fac982538b4ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
