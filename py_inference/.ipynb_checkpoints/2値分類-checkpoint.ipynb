{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0.494314782</th>\n",
       "      <th>0.49401556</th>\n",
       "      <th>0.494314782.1</th>\n",
       "      <th>0.494314782.2</th>\n",
       "      <th>0.494314782.3</th>\n",
       "      <th>0.494314782.4</th>\n",
       "      <th>0.49551167</th>\n",
       "      <th>0.495810892</th>\n",
       "      <th>0.494913226</th>\n",
       "      <th>...</th>\n",
       "      <th>0.425194494.7</th>\n",
       "      <th>0.424895272.14</th>\n",
       "      <th>0.425493716.9</th>\n",
       "      <th>0.425792938.8</th>\n",
       "      <th>0.424296828.17</th>\n",
       "      <th>0.423997606.16</th>\n",
       "      <th>0.425493716.10</th>\n",
       "      <th>0.424895272.15</th>\n",
       "      <th>0.423997606.17</th>\n",
       "      <th>0.425194494.8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.495730</td>\n",
       "      <td>0.495730</td>\n",
       "      <td>0.497225</td>\n",
       "      <td>0.496798</td>\n",
       "      <td>0.494876</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.496157</td>\n",
       "      <td>0.495944</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416524</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.419727</td>\n",
       "      <td>0.419086</td>\n",
       "      <td>0.418232</td>\n",
       "      <td>0.418873</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.417805</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.417378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.423133</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.423133</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386823</td>\n",
       "      <td>0.387701</td>\n",
       "      <td>0.390630</td>\n",
       "      <td>0.389751</td>\n",
       "      <td>0.388873</td>\n",
       "      <td>0.388873</td>\n",
       "      <td>0.387994</td>\n",
       "      <td>0.387994</td>\n",
       "      <td>0.386237</td>\n",
       "      <td>0.386237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.517672</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.515887</td>\n",
       "      <td>0.516244</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.517672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565512</td>\n",
       "      <td>0.565869</td>\n",
       "      <td>0.560514</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.559443</td>\n",
       "      <td>0.557301</td>\n",
       "      <td>0.555516</td>\n",
       "      <td>0.553374</td>\n",
       "      <td>0.554802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.447996</td>\n",
       "      <td>0.448473</td>\n",
       "      <td>0.446565</td>\n",
       "      <td>0.446565</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.447996</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.445134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.477405</td>\n",
       "      <td>0.478514</td>\n",
       "      <td>0.478791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518436</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.520377</td>\n",
       "      <td>0.519823</td>\n",
       "      <td>0.519268</td>\n",
       "      <td>0.519268</td>\n",
       "      <td>0.520932</td>\n",
       "      <td>0.520932</td>\n",
       "      <td>0.521486</td>\n",
       "      <td>0.521209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0.415320</td>\n",
       "      <td>0.415554</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>0.416022</td>\n",
       "      <td>0.416022</td>\n",
       "      <td>0.416257</td>\n",
       "      <td>0.416959</td>\n",
       "      <td>0.416491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342235</td>\n",
       "      <td>0.341298</td>\n",
       "      <td>0.343172</td>\n",
       "      <td>0.344811</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.345046</td>\n",
       "      <td>0.345514</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.344811</td>\n",
       "      <td>0.344343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>0.533198</td>\n",
       "      <td>0.533401</td>\n",
       "      <td>0.532996</td>\n",
       "      <td>0.532591</td>\n",
       "      <td>0.533401</td>\n",
       "      <td>0.533603</td>\n",
       "      <td>0.533198</td>\n",
       "      <td>0.532996</td>\n",
       "      <td>0.533806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.522065</td>\n",
       "      <td>0.522470</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.522672</td>\n",
       "      <td>0.522470</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.521053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>0.387455</td>\n",
       "      <td>0.387058</td>\n",
       "      <td>0.388249</td>\n",
       "      <td>0.388249</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.386264</td>\n",
       "      <td>0.386661</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348551</td>\n",
       "      <td>0.349345</td>\n",
       "      <td>0.352521</td>\n",
       "      <td>0.351330</td>\n",
       "      <td>0.363239</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>0.369194</td>\n",
       "      <td>0.368003</td>\n",
       "      <td>0.373164</td>\n",
       "      <td>0.374355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0.586364</td>\n",
       "      <td>0.586364</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.584091</td>\n",
       "      <td>0.584848</td>\n",
       "      <td>0.585606</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.582576</td>\n",
       "      <td>0.582576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.478030</td>\n",
       "      <td>0.475758</td>\n",
       "      <td>0.474242</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.485606</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.503030</td>\n",
       "      <td>0.505303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622877</td>\n",
       "      <td>0.622877</td>\n",
       "      <td>0.622318</td>\n",
       "      <td>0.621385</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.622504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606270</td>\n",
       "      <td>0.606083</td>\n",
       "      <td>0.610002</td>\n",
       "      <td>0.610002</td>\n",
       "      <td>0.610935</td>\n",
       "      <td>0.611495</td>\n",
       "      <td>0.612054</td>\n",
       "      <td>0.610935</td>\n",
       "      <td>0.609629</td>\n",
       "      <td>0.610935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 12885 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  0.494314782  0.49401556  0.494314782.1  0.494314782.2  0.494314782.3  \\\n",
       "0    1     0.495730    0.495730       0.497225       0.496798       0.494876   \n",
       "1    1     0.423133    0.422840       0.423133       0.423719       0.422840   \n",
       "2    1     0.516601    0.516601       0.516958       0.517672       0.516958   \n",
       "3    1     0.447996    0.448473       0.446565       0.446565       0.447519   \n",
       "4    1     0.479900    0.479623       0.479068       0.479623       0.479900   \n",
       "..  ..          ...         ...            ...            ...            ...   \n",
       "248  0     0.415320    0.415554       0.415788       0.415788       0.416022   \n",
       "249  0     0.533198    0.533401       0.532996       0.532591       0.533401   \n",
       "250  0     0.387455    0.387058       0.388249       0.388249       0.385867   \n",
       "251  0     0.586364    0.586364       0.583333       0.584091       0.584848   \n",
       "252  0     0.622691    0.622691       0.622691       0.622877       0.622877   \n",
       "\n",
       "     0.494314782.4  0.49551167  0.495810892  0.494913226  ...  0.425194494.7  \\\n",
       "0         0.495517    0.496157     0.495944     0.495517  ...       0.416524   \n",
       "1         0.422548    0.423719     0.423719     0.422840  ...       0.386823   \n",
       "2         0.515887    0.516244     0.516958     0.517672  ...       0.565512   \n",
       "3         0.447996    0.447519     0.447519     0.445134  ...       0.000000   \n",
       "4         0.479068    0.477405     0.478514     0.478791  ...       0.518436   \n",
       "..             ...         ...          ...          ...  ...            ...   \n",
       "248       0.416022    0.416257     0.416959     0.416491  ...       0.342235   \n",
       "249       0.533603    0.533198     0.532996     0.533806  ...       0.520243   \n",
       "250       0.386264    0.386661     0.385867     0.385867  ...       0.348551   \n",
       "251       0.585606    0.581818     0.582576     0.582576  ...       0.475000   \n",
       "252       0.622318    0.621385     0.621758     0.622504  ...       0.606270   \n",
       "\n",
       "     0.424895272.14  0.425493716.9  0.425792938.8  0.424296828.17  \\\n",
       "0          0.417592       0.419727       0.419086        0.418232   \n",
       "1          0.387701       0.390630       0.389751        0.388873   \n",
       "2          0.565869       0.560514       0.559800        0.558372   \n",
       "3          0.000000       0.000000       0.000000        0.000000   \n",
       "4          0.518714       0.520377       0.519823        0.519268   \n",
       "..              ...            ...            ...             ...   \n",
       "248        0.341298       0.343172       0.344811        0.346451   \n",
       "249        0.520243       0.522065       0.522470        0.520850   \n",
       "250        0.349345       0.352521       0.351330        0.363239   \n",
       "251        0.478030       0.475758       0.474242        0.472727   \n",
       "252        0.606083       0.610002       0.610002        0.610935   \n",
       "\n",
       "     0.423997606.16  0.425493716.10  0.424895272.15  0.423997606.17  \\\n",
       "0          0.418873        0.417592        0.417805        0.417592   \n",
       "1          0.388873        0.387994        0.387994        0.386237   \n",
       "2          0.559443        0.557301        0.555516        0.553374   \n",
       "3          0.000000        0.000000        0.000000        0.000000   \n",
       "4          0.519268        0.520932        0.520932        0.521486   \n",
       "..              ...             ...             ...             ...   \n",
       "248        0.345046        0.345514        0.346451        0.344811   \n",
       "249        0.520850        0.522672        0.522470        0.520850   \n",
       "250        0.364033        0.369194        0.368003        0.373164   \n",
       "251        0.475000        0.485606        0.483333        0.503030   \n",
       "252        0.611495        0.612054        0.610935        0.609629   \n",
       "\n",
       "     0.425194494.8  \n",
       "0         0.417378  \n",
       "1         0.386237  \n",
       "2         0.554802  \n",
       "3         0.000000  \n",
       "4         0.521209  \n",
       "..             ...  \n",
       "248       0.344343  \n",
       "249       0.521053  \n",
       "250       0.374355  \n",
       "251       0.505303  \n",
       "252       0.610935  \n",
       "\n",
       "[253 rows x 12885 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#d_test = np.array(pd.read_csv('./enter_data_test.csv'))\n",
    "#d_train = np.array(pd.read_csv('./enter_data_train.csv'))\n",
    "df = pd.read_csv('./enter_data.csv')\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.494314782</th>\n",
       "      <th>0.49401556</th>\n",
       "      <th>0.494314782.1</th>\n",
       "      <th>0.494314782.2</th>\n",
       "      <th>0.494314782.3</th>\n",
       "      <th>0.494314782.4</th>\n",
       "      <th>0.49551167</th>\n",
       "      <th>0.495810892</th>\n",
       "      <th>0.494913226</th>\n",
       "      <th>0.494314782.5</th>\n",
       "      <th>...</th>\n",
       "      <th>0.425194494.7</th>\n",
       "      <th>0.424895272.14</th>\n",
       "      <th>0.425493716.9</th>\n",
       "      <th>0.425792938.8</th>\n",
       "      <th>0.424296828.17</th>\n",
       "      <th>0.423997606.16</th>\n",
       "      <th>0.425493716.10</th>\n",
       "      <th>0.424895272.15</th>\n",
       "      <th>0.423997606.17</th>\n",
       "      <th>0.425194494.8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.495730</td>\n",
       "      <td>0.495730</td>\n",
       "      <td>0.497225</td>\n",
       "      <td>0.496798</td>\n",
       "      <td>0.494876</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.496157</td>\n",
       "      <td>0.495944</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416524</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.419727</td>\n",
       "      <td>0.419086</td>\n",
       "      <td>0.418232</td>\n",
       "      <td>0.418873</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.417805</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>0.417378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.423133</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.423133</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.422255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386823</td>\n",
       "      <td>0.387701</td>\n",
       "      <td>0.390630</td>\n",
       "      <td>0.389751</td>\n",
       "      <td>0.388873</td>\n",
       "      <td>0.388873</td>\n",
       "      <td>0.387994</td>\n",
       "      <td>0.387994</td>\n",
       "      <td>0.386237</td>\n",
       "      <td>0.386237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.517672</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.515887</td>\n",
       "      <td>0.516244</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.517672</td>\n",
       "      <td>0.515887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565512</td>\n",
       "      <td>0.565869</td>\n",
       "      <td>0.560514</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.559443</td>\n",
       "      <td>0.557301</td>\n",
       "      <td>0.555516</td>\n",
       "      <td>0.553374</td>\n",
       "      <td>0.554802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.447996</td>\n",
       "      <td>0.448473</td>\n",
       "      <td>0.446565</td>\n",
       "      <td>0.446565</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.447996</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.447519</td>\n",
       "      <td>0.445134</td>\n",
       "      <td>0.444656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.477405</td>\n",
       "      <td>0.478514</td>\n",
       "      <td>0.478791</td>\n",
       "      <td>0.478514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518436</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.520377</td>\n",
       "      <td>0.519823</td>\n",
       "      <td>0.519268</td>\n",
       "      <td>0.519268</td>\n",
       "      <td>0.520932</td>\n",
       "      <td>0.520932</td>\n",
       "      <td>0.521486</td>\n",
       "      <td>0.521209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.415320</td>\n",
       "      <td>0.415554</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>0.416022</td>\n",
       "      <td>0.416022</td>\n",
       "      <td>0.416257</td>\n",
       "      <td>0.416959</td>\n",
       "      <td>0.416491</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342235</td>\n",
       "      <td>0.341298</td>\n",
       "      <td>0.343172</td>\n",
       "      <td>0.344811</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.345046</td>\n",
       "      <td>0.345514</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.344811</td>\n",
       "      <td>0.344343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.533198</td>\n",
       "      <td>0.533401</td>\n",
       "      <td>0.532996</td>\n",
       "      <td>0.532591</td>\n",
       "      <td>0.533401</td>\n",
       "      <td>0.533603</td>\n",
       "      <td>0.533198</td>\n",
       "      <td>0.532996</td>\n",
       "      <td>0.533806</td>\n",
       "      <td>0.533806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.520243</td>\n",
       "      <td>0.522065</td>\n",
       "      <td>0.522470</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.522672</td>\n",
       "      <td>0.522470</td>\n",
       "      <td>0.520850</td>\n",
       "      <td>0.521053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.387455</td>\n",
       "      <td>0.387058</td>\n",
       "      <td>0.388249</td>\n",
       "      <td>0.388249</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.386264</td>\n",
       "      <td>0.386661</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.385867</td>\n",
       "      <td>0.386661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348551</td>\n",
       "      <td>0.349345</td>\n",
       "      <td>0.352521</td>\n",
       "      <td>0.351330</td>\n",
       "      <td>0.363239</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>0.369194</td>\n",
       "      <td>0.368003</td>\n",
       "      <td>0.373164</td>\n",
       "      <td>0.374355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.586364</td>\n",
       "      <td>0.586364</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.584091</td>\n",
       "      <td>0.584848</td>\n",
       "      <td>0.585606</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.582576</td>\n",
       "      <td>0.582576</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.478030</td>\n",
       "      <td>0.475758</td>\n",
       "      <td>0.474242</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.485606</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.503030</td>\n",
       "      <td>0.505303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622691</td>\n",
       "      <td>0.622877</td>\n",
       "      <td>0.622877</td>\n",
       "      <td>0.622318</td>\n",
       "      <td>0.621385</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.622504</td>\n",
       "      <td>0.621571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606270</td>\n",
       "      <td>0.606083</td>\n",
       "      <td>0.610002</td>\n",
       "      <td>0.610002</td>\n",
       "      <td>0.610935</td>\n",
       "      <td>0.611495</td>\n",
       "      <td>0.612054</td>\n",
       "      <td>0.610935</td>\n",
       "      <td>0.609629</td>\n",
       "      <td>0.610935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 12884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.494314782  0.49401556  0.494314782.1  0.494314782.2  0.494314782.3  \\\n",
       "0       0.495730    0.495730       0.497225       0.496798       0.494876   \n",
       "1       0.423133    0.422840       0.423133       0.423719       0.422840   \n",
       "2       0.516601    0.516601       0.516958       0.517672       0.516958   \n",
       "3       0.447996    0.448473       0.446565       0.446565       0.447519   \n",
       "4       0.479900    0.479623       0.479068       0.479623       0.479900   \n",
       "..           ...         ...            ...            ...            ...   \n",
       "248     0.415320    0.415554       0.415788       0.415788       0.416022   \n",
       "249     0.533198    0.533401       0.532996       0.532591       0.533401   \n",
       "250     0.387455    0.387058       0.388249       0.388249       0.385867   \n",
       "251     0.586364    0.586364       0.583333       0.584091       0.584848   \n",
       "252     0.622691    0.622691       0.622691       0.622877       0.622877   \n",
       "\n",
       "     0.494314782.4  0.49551167  0.495810892  0.494913226  0.494314782.5  ...  \\\n",
       "0         0.495517    0.496157     0.495944     0.495517       0.495517  ...   \n",
       "1         0.422548    0.423719     0.423719     0.422840       0.422255  ...   \n",
       "2         0.515887    0.516244     0.516958     0.517672       0.515887  ...   \n",
       "3         0.447996    0.447519     0.447519     0.445134       0.444656  ...   \n",
       "4         0.479068    0.477405     0.478514     0.478791       0.478514  ...   \n",
       "..             ...         ...          ...          ...            ...  ...   \n",
       "248       0.416022    0.416257     0.416959     0.416491       0.415788  ...   \n",
       "249       0.533603    0.533198     0.532996     0.533806       0.533806  ...   \n",
       "250       0.386264    0.386661     0.385867     0.385867       0.386661  ...   \n",
       "251       0.585606    0.581818     0.582576     0.582576       0.581818  ...   \n",
       "252       0.622318    0.621385     0.621758     0.622504       0.621571  ...   \n",
       "\n",
       "     0.425194494.7  0.424895272.14  0.425493716.9  0.425792938.8  \\\n",
       "0         0.416524        0.417592       0.419727       0.419086   \n",
       "1         0.386823        0.387701       0.390630       0.389751   \n",
       "2         0.565512        0.565869       0.560514       0.559800   \n",
       "3         0.000000        0.000000       0.000000       0.000000   \n",
       "4         0.518436        0.518714       0.520377       0.519823   \n",
       "..             ...             ...            ...            ...   \n",
       "248       0.342235        0.341298       0.343172       0.344811   \n",
       "249       0.520243        0.520243       0.522065       0.522470   \n",
       "250       0.348551        0.349345       0.352521       0.351330   \n",
       "251       0.475000        0.478030       0.475758       0.474242   \n",
       "252       0.606270        0.606083       0.610002       0.610002   \n",
       "\n",
       "     0.424296828.17  0.423997606.16  0.425493716.10  0.424895272.15  \\\n",
       "0          0.418232        0.418873        0.417592        0.417805   \n",
       "1          0.388873        0.388873        0.387994        0.387994   \n",
       "2          0.558372        0.559443        0.557301        0.555516   \n",
       "3          0.000000        0.000000        0.000000        0.000000   \n",
       "4          0.519268        0.519268        0.520932        0.520932   \n",
       "..              ...             ...             ...             ...   \n",
       "248        0.346451        0.345046        0.345514        0.346451   \n",
       "249        0.520850        0.520850        0.522672        0.522470   \n",
       "250        0.363239        0.364033        0.369194        0.368003   \n",
       "251        0.472727        0.475000        0.485606        0.483333   \n",
       "252        0.610935        0.611495        0.612054        0.610935   \n",
       "\n",
       "     0.423997606.17  0.425194494.8  \n",
       "0          0.417592       0.417378  \n",
       "1          0.386237       0.386237  \n",
       "2          0.553374       0.554802  \n",
       "3          0.000000       0.000000  \n",
       "4          0.521486       0.521209  \n",
       "..              ...            ...  \n",
       "248        0.344811       0.344343  \n",
       "249        0.520850       0.521053  \n",
       "250        0.373164       0.374355  \n",
       "251        0.503030       0.505303  \n",
       "252        0.609629       0.610935  \n",
       "\n",
       "[253 rows x 12884 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test  = df.iloc[:,1:]\n",
    "x_lavel = df.iloc[:,0]\n",
    "\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 12884)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x_test = np.array(x_test)\n",
    "_x_lavel = np.array(x_lavel)\n",
    "\n",
    "_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:1,y_one_hot:[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "x_one_hot = np_utils.to_categorical(x_lavel)\n",
    "print(f\"y:{x_lavel[0]},y_one_hot:{x_one_hot[0]}\")\n",
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 教師データとテストデータに分割\n",
    "x_train, x_test, y_train, y_test= train_test_split(x_test, x_one_hot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# モデルの作成\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=12884))    # 入力層12884ノード, 隠れ層に3ノード, 全結合\n",
    "model.add(Activation(\"relu\"))    # 活性化関数はsigmoid\n",
    "model.add(Dense(2)) # 出力層2ノード,全結合\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",   # 誤差関数 binary_crossentropy categorical_crossentropy\n",
    "              optimizer=Adam(lr=0.01),     # 最適化手法adam sgd\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.6750 - accuracy: 0.6832\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.7129\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6890 - accuracy: 0.7129\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6869 - accuracy: 0.7129\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6849 - accuracy: 0.7129\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6828 - accuracy: 0.7129\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6809 - accuracy: 0.7129\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6789 - accuracy: 0.7129\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6770 - accuracy: 0.7129\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6751 - accuracy: 0.7129\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6732 - accuracy: 0.7129\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6713 - accuracy: 0.7129\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6695 - accuracy: 0.7129\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6677 - accuracy: 0.7129\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6660 - accuracy: 0.7129\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6642 - accuracy: 0.7129\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6625 - accuracy: 0.7129\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6608 - accuracy: 0.7129\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6592 - accuracy: 0.7129\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6576 - accuracy: 0.7129\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6560 - accuracy: 0.7129\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.7129\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6529 - accuracy: 0.7129\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6514 - accuracy: 0.7129\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6500 - accuracy: 0.7129\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6486 - accuracy: 0.7129\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6472 - accuracy: 0.7129\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6458 - accuracy: 0.7129\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6444 - accuracy: 0.7129\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6431 - accuracy: 0.7129\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6418 - accuracy: 0.7129\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6406 - accuracy: 0.7129\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6394 - accuracy: 0.7129\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6382 - accuracy: 0.7129\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6370 - accuracy: 0.7129\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6358 - accuracy: 0.7129\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6347 - accuracy: 0.7129\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6336 - accuracy: 0.7129\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6326 - accuracy: 0.7129\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6315 - accuracy: 0.7129\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6305 - accuracy: 0.7129\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6295 - accuracy: 0.7129\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6286 - accuracy: 0.7129\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6276 - accuracy: 0.7129\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6267 - accuracy: 0.7129\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6258 - accuracy: 0.7129\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6250 - accuracy: 0.7129\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6241 - accuracy: 0.7129\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6233 - accuracy: 0.7129\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6225 - accuracy: 0.7129\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6217 - accuracy: 0.7129\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6210 - accuracy: 0.7129\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6203 - accuracy: 0.7129\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6196 - accuracy: 0.7129\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6189 - accuracy: 0.7129\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6182 - accuracy: 0.7129\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6175 - accuracy: 0.7129\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6169 - accuracy: 0.7129\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6163 - accuracy: 0.7129\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6157 - accuracy: 0.7129\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6151 - accuracy: 0.7129\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6146 - accuracy: 0.7129\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6140 - accuracy: 0.7129\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6135 - accuracy: 0.7129\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6130 - accuracy: 0.7129\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6125 - accuracy: 0.7129\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6120 - accuracy: 0.7129\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6116 - accuracy: 0.7129\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6111 - accuracy: 0.7129\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6107 - accuracy: 0.7129\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6103 - accuracy: 0.7129\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6098 - accuracy: 0.7129\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.7129\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6091 - accuracy: 0.7129\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.7129\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6084 - accuracy: 0.7129\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6080 - accuracy: 0.7129\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6077 - accuracy: 0.7129\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6074 - accuracy: 0.7129\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6070 - accuracy: 0.7129\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6067 - accuracy: 0.7129\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6065 - accuracy: 0.7129\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6062 - accuracy: 0.7129\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6059 - accuracy: 0.7129\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6057 - accuracy: 0.7129\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6054 - accuracy: 0.7129\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6052 - accuracy: 0.7129\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6049 - accuracy: 0.7129\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6047 - accuracy: 0.7129\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6045 - accuracy: 0.7129\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6043 - accuracy: 0.7129\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6041 - accuracy: 0.7129\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6039 - accuracy: 0.7129\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6037 - accuracy: 0.7129\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6035 - accuracy: 0.7129\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6034 - accuracy: 0.7129\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6032 - accuracy: 0.7129\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6030 - accuracy: 0.7129\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6029 - accuracy: 0.7129\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6027 - accuracy: 0.7129\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6026 - accuracy: 0.7129\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6025 - accuracy: 0.7129\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6023 - accuracy: 0.7129\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6022 - accuracy: 0.7129\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6021 - accuracy: 0.7129\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6020 - accuracy: 0.7129\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6019 - accuracy: 0.7129\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6018 - accuracy: 0.7129\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6017 - accuracy: 0.7129\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6016 - accuracy: 0.7129\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6015 - accuracy: 0.7129\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6014 - accuracy: 0.7129\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6013 - accuracy: 0.7129\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6012 - accuracy: 0.7129\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6011 - accuracy: 0.7129\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6011 - accuracy: 0.7129\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6010 - accuracy: 0.7129\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6009 - accuracy: 0.7129\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6009 - accuracy: 0.7129\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6008 - accuracy: 0.7129\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6007 - accuracy: 0.7129\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6007 - accuracy: 0.7129\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6006 - accuracy: 0.7129\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6006 - accuracy: 0.7129\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6005 - accuracy: 0.7129\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6005 - accuracy: 0.7129\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6004 - accuracy: 0.7129\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6004 - accuracy: 0.7129\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6003 - accuracy: 0.7129\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6003 - accuracy: 0.7129\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6003 - accuracy: 0.7129\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6002 - accuracy: 0.7129\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6002 - accuracy: 0.7129\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6002 - accuracy: 0.7129\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.7129\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.7129\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6001 - accuracy: 0.7129\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6000 - accuracy: 0.7129\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6000 - accuracy: 0.7129\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6000 - accuracy: 0.7129\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6000 - accuracy: 0.7129\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6000 - accuracy: 0.7129\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5999 - accuracy: 0.7129\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.7129\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5999 - accuracy: 0.7129\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.7129\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5999 - accuracy: 0.7129\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7129\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5997 - accuracy: 0.7129\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5996 - accuracy: 0.7129\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5996 - accuracy: 0.7129\n"
     ]
    }
   ],
   "source": [
    "# 訓練\n",
    "# 約数 1　2　4　3221　6442　12884\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=3221) # 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5cecd6e430>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"accuracy\"], label=\"acc\")\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6547 - accuracy: 0.7255\n",
      "Test score 0.6547414660453796\n",
      "Test accuracy 0.7254902124404907\n"
     ]
    }
   ],
   "source": [
    "# 評価\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test score\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('enter_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5cf5903e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6547 - accuracy: 0.7255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6547414660453796, 0.7254902124404907]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "load_model = keras.models.load_model('enter_model')\n",
    "\n",
    "load_model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.5103 - acc: 0.7911 - val_loss: 0.4016 - val_acc: 0.8628\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.3110 - acc: 0.9031 - val_loss: 0.3085 - val_acc: 0.8870\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.2309 - acc: 0.9235 - val_loss: 0.2803 - val_acc: 0.8908\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.1795 - acc: 0.9428 - val_loss: 0.2735 - val_acc: 0.8893\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.1475 - acc: 0.9526 - val_loss: 0.2788 - val_acc: 0.8890\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.1185 - acc: 0.9638 - val_loss: 0.3330 - val_acc: 0.8764\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.1005 - acc: 0.9703 - val_loss: 0.3055 - val_acc: 0.8838\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0818 - acc: 0.9773 - val_loss: 0.3344 - val_acc: 0.8769\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0696 - acc: 0.9814 - val_loss: 0.3607 - val_acc: 0.8800\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0547 - acc: 0.9873 - val_loss: 0.3776 - val_acc: 0.8785\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0453 - acc: 0.9895 - val_loss: 0.4035 - val_acc: 0.8765\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0353 - acc: 0.9930 - val_loss: 0.4437 - val_acc: 0.8766\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0269 - acc: 0.9956 - val_loss: 0.4637 - val_acc: 0.8747\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0212 - acc: 0.9968 - val_loss: 0.4877 - val_acc: 0.8714\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0162 - acc: 0.9977 - val_loss: 0.6080 - val_acc: 0.8625\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0115 - acc: 0.9993 - val_loss: 0.5778 - val_acc: 0.8698\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0116 - acc: 0.9979 - val_loss: 0.5906 - val_acc: 0.8702\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0054 - acc: 0.9998 - val_loss: 0.6204 - val_acc: 0.8639\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0083 - acc: 0.9984 - val_loss: 0.6419 - val_acc: 0.8676\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s - loss: 0.0031 - acc: 0.9998 - val_loss: 0.6796 - val_acc: 0.8683\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_acc', 'acc', 'val_loss', 'loss'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
